{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e174b1a1",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "This notebook demonstrates a single model which combines paid and reported losses that converge to the same ultimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655305f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import pyro.distributions as dist\n",
    "import pyro\n",
    "from pyro.infer.reparam import SplitReparam\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b6b41",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "\n",
    "The base line model is defined using the per development age means and standard deviations of the loss development factors. Note that the first values is an actual loss value, and not a loss development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ce526",
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_loss_development_factor_mean = pt.Tensor([0.3, 3.0, 1.8, 1.2, 1.1])\n",
    "paid_loss_development_factor_std  = pt.Tensor([2.0, 2.0, 1.0, 0.2, 0.1])\n",
    "reported_loss_development_factor_mean = pt.Tensor([0.6, 1.3, 1.2, 1.1, 1.03])\n",
    "reported_loss_development_factor_std  = pt.Tensor([0.1, 0.2, 0.1, 0.05, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc15ea5",
   "metadata": {},
   "source": [
    "We now figure out the parameters of the log-normal distribution of the loss development factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_z_params(mu, sigma):\n",
    "    return pt.log(mu * mu / pt.sqrt(mu * mu + sigma * sigma)), \\\n",
    "           pt.sqrt(pt.log(1 + sigma * sigma / mu * mu))\n",
    "\n",
    "paid_loss_development_factor_mu, paid_loss_development_factor_sigma = \\\n",
    "    log_normal_z_params(paid_loss_development_factor_mean,paid_loss_development_factor_std)\n",
    "reported_loss_development_factor_mu, reported_loss_development_factor_sigma = \\\n",
    "    log_normal_z_params(reported_loss_development_factor_mean, reported_loss_development_factor_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c753a9",
   "metadata": {},
   "source": [
    "Below is a definition of the transformation which converts loss development factors into the cumulative loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6022a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CumSumTransform(dist.transforms.Transform):\n",
    "    domain = dist.constraints.independent(dist.constraints.real, 1)\n",
    "    codomain = dist.constraints.independent(dist.constraints.real, 1)\n",
    "    bijective = True\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return x.new_zeros(x.shape[:(-1)])\n",
    "\n",
    "    def _call(self, x):\n",
    "        return x.cumsum(dim=-1)\n",
    "\n",
    "    def _inverse(self, x):\n",
    "        return pt.diff(x, prepend=x.new_zeros(x.shape[:-1] +(1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f838543",
   "metadata": {},
   "source": [
    "Let's define our loss distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44091e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss development distributions\n",
    "paid_loss_development_factor_dist = dist.Normal(paid_loss_development_factor_mu, paid_loss_development_factor_sigma)\n",
    "reported_loss_development_factor_dist = dist.Normal(reported_loss_development_factor_mu, reported_loss_development_factor_sigma)\n",
    "\n",
    "# Cumulative loss distributions\n",
    "paid_loss_dist = dist.TransformedDistribution(paid_loss_development_factor_dist,\n",
    "                                                  [CumSumTransform(), dist.transforms.ExpTransform()])\n",
    "reported_loss_dist = dist.TransformedDistribution(reported_loss_development_factor_dist,\n",
    "                                                  [CumSumTransform(), dist.transforms.ExpTransform()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d6451",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b218ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    paid_loss = pyro.sample(\"paid_loss\", paid_loss_dist)\n",
    "    reported_loss = pyro.sample(\"reported_loss\", reported_loss_dist)\n",
    "    return paid_loss, reported_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb8ef8",
   "metadata": {},
   "source": [
    "We use Stochastic Variational Inference (SVI) to approximate the base line models distribution. SVI is not needed for the base line model but we will use later on for the same ultimate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, num_steps=501, lr=0.1):\n",
    "    pyro.clear_param_store()\n",
    "    guide = pyro.infer.autoguide.AutoMultivariateNormal(model)\n",
    "    # Setup ELBO and optimizer\n",
    "    elbo = pyro.infer.Trace_ELBO(vectorize_particles=True, num_particles=100)\n",
    "    elbo.loss(model, guide)\n",
    "    optim = pyro.optim.Adam({\"lr\": lr})\n",
    "    svi = pyro.infer.SVI(model, guide, optim, loss=elbo)\n",
    "\n",
    "    # optimize\n",
    "    for i in range(num_steps):\n",
    "        loss = svi.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"step {i} loss = {loss:0.6g}\")\n",
    "\n",
    "    return guide\n",
    "\n",
    "base_guide = fit(base_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c830e4",
   "metadata": {},
   "source": [
    "Let's sample and plot the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ac331",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_samples = pyro.infer.WeighedPredictive(base_model, guide=base_guide, num_samples=10000,\n",
    "                                            parallel=True, return_sites=[\"paid_loss\", \"reported_loss\"])()\n",
    "\n",
    "lower_percentile = 0.25\n",
    "upper_percentile = 0.75\n",
    "\n",
    "def plot_ci(samples, **kwargs):\n",
    "    lower, upper = pyro.ops.stats.quantile(samples, [lower_percentile, upper_percentile])\n",
    "    plt.fill_between(range(samples.shape[-1]), lower, upper, **kwargs)\n",
    "\n",
    "plot_ci(base_samples.samples[\"paid_loss\"], alpha=0.5, label=\"Paid\")\n",
    "plot_ci(base_samples.samples[\"reported_loss\"], alpha=0.5, label=\"Reported\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ee572",
   "metadata": {},
   "source": [
    "We can observe that the last age paid and reported losses are not the same, as they have different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7053f",
   "metadata": {},
   "source": [
    "# Same ultimate model\n",
    "\n",
    "The same ultimate model is derived from the baseline model by conditioning it, requiring that the last age paid and reported losses are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40223725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split last loss\n",
    "split_reparam = SplitReparam([len(paid_loss_development_factor_mu) - 1, 1], -1)\n",
    "split_model = pyro.poutine.reparam(base_model, dict(paid_loss=split_reparam, reported_loss=split_reparam))\n",
    "# Equalize last loss\n",
    "same_utimate_model = pyro.poutine.equalize(split_model, [\"paid_loss_split_1\", \"reported_loss_split_1\"])\n",
    "same_ultimate_guide = fit(same_utimate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6b563",
   "metadata": {},
   "source": [
    "Let's sample the same ultimate model and plot the loss distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_ultimate_samples = pyro.infer.WeighedPredictive(same_utimate_model, guide=same_ultimate_guide, num_samples=10000,\n",
    "                                                     parallel=True, return_sites=[\"paid_loss\", \"reported_loss\"])()\n",
    "\n",
    "plot_ci(same_ultimate_samples.samples[\"paid_loss\"], alpha=0.5, label=\"Paid\")\n",
    "plot_ci(same_ultimate_samples.samples[\"reported_loss\"], alpha=0.5, label=\"Reported\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c616e",
   "metadata": {},
   "source": [
    "We can see how the conditioning of the model changed the loss distributions. Now the last age paid and reported losses have the same distribution, and they are in fact identical (you can look at the samples in order to verify this).\n",
    "\n",
    "We can now look at all four distributions in order to get a sense of the conditioning changed our original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcde605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ci(same_ultimate_samples.samples[\"paid_loss\"], alpha=0.5, label=\"Same Ultimate Paid\")\n",
    "plot_ci(same_ultimate_samples.samples[\"reported_loss\"], alpha=0.5, label=\"Same Ultimate Reported\")\n",
    "plot_ci(base_samples.samples[\"paid_loss\"], alpha=0.5, label=\"Baseline Paid\")\n",
    "plot_ci(base_samples.samples[\"reported_loss\"], alpha=0.5, label=\"Baseline Reported\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c797ce7",
   "metadata": {},
   "source": [
    "Note how the confidence interval of the same ultimate model paid losses starts becoming narrower as the age increases from the second age onwards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
